[[34m2024-04-29T08:48:54.930+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-04-29T08:48:54.939+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-29T08:48:54.951+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 34695[0m
[[34m2024-04-29T08:48:54.953+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-29T08:48:54.966+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-29T08:48:55.033+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-29T08:49:27.259+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:49:25.125693+00:00 [scheduled]>[0m
[[34m2024-04-29T08:49:27.259+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:49:27.259+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:49:25.125693+00:00 [scheduled]>[0m
[[34m2024-04-29T08:49:27.316+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-04-29T08:49:25.125693+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-29T08:49:27.316+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-04-29T08:49:25.125693+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:49:27.338+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-04-29T08:49:25.125693+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:49:28.377+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-04-29T08:49:29.065+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:49:25.125693+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:49:29.685+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-04-29T08:49:25.125693+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:49:29.691+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_0, run_id=manual__2024-04-29T08:49:25.125693+00:00, map_index=-1, run_start_date=2024-04-29 08:49:29.146685+00:00, run_end_date=2024-04-29 08:49:29.362148+00:00, run_duration=0.215463, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-04-29 08:49:27.260328+00:00, queued_by_job_id=7, pid=34911[0m
[[34m2024-04-29T08:49:29.752+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:49:25.125693+00:00 [scheduled]>[0m
[[34m2024-04-29T08:49:29.752+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:49:29.752+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:49:25.125693+00:00 [scheduled]>[0m
[[34m2024-04-29T08:49:29.754+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-04-29T08:49:25.125693+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T08:49:29.754+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-04-29T08:49:25.125693+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:49:29.776+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-04-29T08:49:25.125693+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:49:30.605+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-04-29T08:49:31.164+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:49:25.125693+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:49:36.813+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-04-29T08:49:25.125693+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:49:36.817+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_1, run_id=manual__2024-04-29T08:49:25.125693+00:00, map_index=-1, run_start_date=2024-04-29 08:49:31.233239+00:00, run_end_date=2024-04-29 08:49:36.460872+00:00, run_duration=5.227633, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-29 08:49:29.753378+00:00, queued_by_job_id=7, pid=34920[0m
[[34m2024-04-29T08:49:36.883+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun two_task_dag @ 2024-04-29 08:49:25.125693+00:00: manual__2024-04-29T08:49:25.125693+00:00, state:running, queued_at: 2024-04-29 08:49:25.142141+00:00. externally triggered: True> successful[0m
[[34m2024-04-29T08:49:36.884+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=two_task_dag, execution_date=2024-04-29 08:49:25.125693+00:00, run_id=manual__2024-04-29T08:49:25.125693+00:00, run_start_date=2024-04-29 08:49:27.033422+00:00, run_end_date=2024-04-29 08:49:36.884476+00:00, run_duration=9.851054, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-29 08:49:25.125693+00:00, data_interval_end=2024-04-29 08:49:25.125693+00:00, dag_hash=9b634b042aeba58c15c1a65bfc911c15[0m
[[34m2024-04-29T08:50:37.268+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:50:36.733494+00:00 [scheduled]>[0m
[[34m2024-04-29T08:50:37.269+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:50:37.269+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:50:36.733494+00:00 [scheduled]>[0m
[[34m2024-04-29T08:50:37.270+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-04-29T08:50:36.733494+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-29T08:50:37.271+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-04-29T08:50:36.733494+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:50:37.293+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-04-29T08:50:36.733494+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:50:38.135+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-04-29T08:50:38.753+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_0 manual__2024-04-29T08:50:36.733494+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:50:39.413+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-04-29T08:50:36.733494+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:50:39.417+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_0, run_id=manual__2024-04-29T08:50:36.733494+00:00, map_index=-1, run_start_date=2024-04-29 08:50:38.824314+00:00, run_end_date=2024-04-29 08:50:39.053652+00:00, run_duration=0.229338, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-04-29 08:50:37.269868+00:00, queued_by_job_id=7, pid=35334[0m
[[34m2024-04-29T08:50:39.521+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:50:36.733494+00:00 [scheduled]>[0m
[[34m2024-04-29T08:50:39.521+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:50:39.521+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:50:36.733494+00:00 [scheduled]>[0m
[[34m2024-04-29T08:50:39.523+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-04-29T08:50:36.733494+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T08:50:39.523+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-04-29T08:50:36.733494+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:50:39.546+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-04-29T08:50:36.733494+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-04-29T08:50:40.376+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-04-29T08:50:40.994+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_1 manual__2024-04-29T08:50:36.733494+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:50:46.620+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-04-29T08:50:36.733494+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:50:46.624+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_1, run_id=manual__2024-04-29T08:50:36.733494+00:00, map_index=-1, run_start_date=2024-04-29 08:50:41.063743+00:00, run_end_date=2024-04-29 08:50:46.282212+00:00, run_duration=5.218469, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-29 08:50:39.522320+00:00, queued_by_job_id=7, pid=35349[0m
[[34m2024-04-29T08:50:46.685+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun two_task_dag @ 2024-04-29 08:50:36.733494+00:00: manual__2024-04-29T08:50:36.733494+00:00, state:running, queued_at: 2024-04-29 08:50:36.742872+00:00. externally triggered: True> successful[0m
[[34m2024-04-29T08:50:46.685+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=two_task_dag, execution_date=2024-04-29 08:50:36.733494+00:00, run_id=manual__2024-04-29T08:50:36.733494+00:00, run_start_date=2024-04-29 08:50:37.190457+00:00, run_end_date=2024-04-29 08:50:46.685830+00:00, run_duration=9.495373, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-29 08:50:36.733494+00:00, data_interval_end=2024-04-29 08:50:36.733494+00:00, dag_hash=9b634b042aeba58c15c1a65bfc911c15[0m
