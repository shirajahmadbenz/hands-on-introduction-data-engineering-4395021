[[34m2024-04-29T08:17:32.188+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-04-29T08:17:32.189+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-29T08:17:32.196+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 21456[0m
[[34m2024-04-29T08:17:32.198+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-04-29T08:17:32.213+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-04-29T08:17:32.271+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-29T08:19:06.295+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:19:04.528297+00:00 [scheduled]>[0m
[[34m2024-04-29T08:19:06.295+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:19:06.295+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:19:04.528297+00:00 [scheduled]>[0m
[[34m2024-04-29T08:19:06.298+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:19:04.528297+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T08:19:06.298+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:19:04.528297+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:19:06.320+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:19:04.528297+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:19:07.354+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-04-29T08:19:08.025+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-04-29T08:19:04.528297+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:19:08.629+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:19:04.528297+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:19:08.635+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-04-29T08:19:04.528297+00:00, map_index=-1, run_start_date=2024-04-29 08:19:08.102042+00:00, run_end_date=2024-04-29 08:19:08.300313+00:00, run_duration=0.198271, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-29 08:19:06.296509+00:00, queued_by_job_id=3, pid=22049[0m
[[34m2024-04-29T08:19:08.694+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun one_task_dag @ 2024-04-29 08:19:04.528297+00:00: manual__2024-04-29T08:19:04.528297+00:00, state:running, queued_at: 2024-04-29 08:19:04.570719+00:00. externally triggered: True> failed[0m
[[34m2024-04-29T08:19:08.695+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-04-29 08:19:04.528297+00:00, run_id=manual__2024-04-29T08:19:04.528297+00:00, run_start_date=2024-04-29 08:19:06.189682+00:00, run_end_date=2024-04-29 08:19:08.695521+00:00, run_duration=2.505839, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-29 08:19:04.528297+00:00, data_interval_end=2024-04-29 08:19:04.528297+00:00, dag_hash=eb01ab0633fbc6180de99241b1f30a47[0m
[[34m2024-04-29T08:20:19.546+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:20:18.180324+00:00 [scheduled]>[0m
[[34m2024-04-29T08:20:19.546+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:20:19.547+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:20:18.180324+00:00 [scheduled]>[0m
[[34m2024-04-29T08:20:19.548+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:20:18.180324+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T08:20:19.548+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:20:18.180324+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:20:19.571+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:20:18.180324+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:20:20.666+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-04-29T08:20:21.565+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-04-29T08:20:18.180324+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:20:22.189+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:20:18.180324+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:20:22.193+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-04-29T08:20:18.180324+00:00, map_index=-1, run_start_date=2024-04-29 08:20:21.642071+00:00, run_end_date=2024-04-29 08:20:21.845499+00:00, run_duration=0.203428, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-29 08:20:19.547684+00:00, queued_by_job_id=3, pid=22503[0m
[[34m2024-04-29T08:20:22.267+0000[0m] {[34mdagrun.py:[0m609} ERROR[0m - Marking run <DagRun one_task_dag @ 2024-04-29 08:20:18.180324+00:00: manual__2024-04-29T08:20:18.180324+00:00, state:running, queued_at: 2024-04-29 08:20:18.186501+00:00. externally triggered: True> failed[0m
[[34m2024-04-29T08:20:22.267+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-04-29 08:20:18.180324+00:00, run_id=manual__2024-04-29T08:20:18.180324+00:00, run_start_date=2024-04-29 08:20:19.478239+00:00, run_end_date=2024-04-29 08:20:22.267797+00:00, run_duration=2.789558, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-04-29 08:20:18.180324+00:00, data_interval_end=2024-04-29 08:20:18.180324+00:00, dag_hash=eb01ab0633fbc6180de99241b1f30a47[0m
[[34m2024-04-29T08:21:40.463+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:21:39.514417+00:00 [scheduled]>[0m
[[34m2024-04-29T08:21:40.463+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-04-29T08:21:40.463+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-04-29T08:21:39.514417+00:00 [scheduled]>[0m
[[34m2024-04-29T08:21:40.465+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:21:39.514417+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T08:21:40.465+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:21:39.514417+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:21:40.490+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-04-29T08:21:39.514417+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-04-29T08:21:41.684+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-04-29T08:21:42.371+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-04-29T08:21:39.514417+00:00 [queued]> on host codespaces-910724[0m
[[34m2024-04-29T08:21:43.203+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-04-29T08:21:39.514417+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T08:21:43.206+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-04-29T08:21:39.514417+00:00, map_index=-1, run_start_date=2024-04-29 08:21:42.514205+00:00, run_end_date=2024-04-29 08:21:42.881041+00:00, run_duration=0.366836, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-04-29 08:21:40.464227+00:00, queued_by_job_id=3, pid=23001[0m
[[34m2024-04-29T08:21:43.241+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun one_task_dag @ 2024-04-29 08:21:39.514417+00:00: manual__2024-04-29T08:21:39.514417+00:00, state:running, queued_at: 2024-04-29 08:21:39.520836+00:00. externally triggered: True> successful[0m
[[34m2024-04-29T08:21:43.241+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-04-29 08:21:39.514417+00:00, run_id=manual__2024-04-29T08:21:39.514417+00:00, run_start_date=2024-04-29 08:21:40.408218+00:00, run_end_date=2024-04-29 08:21:43.241517+00:00, run_duration=2.833299, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-29 08:21:39.514417+00:00, data_interval_end=2024-04-29 08:21:39.514417+00:00, dag_hash=a7273cd1ee170590b18d99bc8f4428ac[0m
[[34m2024-04-29T08:22:32.679+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
